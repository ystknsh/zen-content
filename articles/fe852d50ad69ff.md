---
title: "国会図書館データをGoogle Colab で取得し、Hugging Face へアップロードする方法"
emoji: "📚"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LLM", "GENIAC", "松尾研", "huggingface", "国会図書館"]
published: true
publication_name: matsuolab
published_at: 2024-09-20 06:00 # 未来の日時を指定する
---

# 1. はじめに
NEDO[^1] GENIAC 東京大学 松尾・岩澤研究室 LLM開発プロジェクト[^2] において Team Tanuki の事前学習データ収集の一環として、国会図書館デジタルコレクション[^3]のデータをダウンロードし Hugging Face へアップロードしました。本記事では実際に用いたソースコードを元に、国会図書館デジタルコレクションデータの取得時のポイントについて説明します。

[^1]: NEDO 国立研究開発法人新エネルギー・産業技術総合開発機構
[^2]: GENIAC 松尾研 LLM開発プロジェクト  
    https://weblab.t.u-tokyo.ac.jp/geniac_llm/

[^3]: 国会図書館デジタルコレクション  
    https://dl.ndl.go.jp

# 2. ソースコード
ソースコードはこちらからダウンロードできます。このコードはチームメンバー [林寛太](https://zenn.dev/misdelivery)さんのものをベースにして作成したものです。  
https://github.com/matsuolab/nedo_project_code/blob/team_hatakeyama_phase2/team_hatakeyama_phase2/pre_training_data/ndlj/text_download_NationalDietLibraryJapan.ipynb

このコードは次のように処理されます。

1. データの読み込み: 書誌情報データセット(Excel) を読み込みます。
2. データのダウンロード: 国会図書館デジタルコレクションから書誌データを一つずつダウンロードします。
3. データの保存: ダウンロードしたデータを "本文" という列で保存します。
4. データのアップロード: 処理したデータを Hugging Face へアップロードします。

このコードを実行した後のフォルダ構造はこのようになります
- `working_folder`
  - `text_files`
    - `{Excel File Name}`
    - `{Excel File Name}`
    - etc...
  - `csv_files`
    - batch 毎にcsv ファイルを保存

# 3. 事前準備
## 3.1. 書誌情報データセットの入手
国会図書館オープンデータセット[^4] から書誌情報データセット(Excel ファイル)をダウンロードします。  
今回のプロジェクトでは以下4つの書誌情報データセットを利用しました。

- 図書 x 2ファイル
- 古典籍
- 雑誌

[^4]: 国会図書館オープンデータセット  
    https://www.ndl.go.jp/jp/dlib/standards/opendataset/index.html

## 3.2. 動作環境の準備
- Google Colab Pro
- GPU: 不要
- メモリ: `ハイメモリ`モード (50GB 程度必要)
- ディスク空き容量: 400GB 程度

## 3.3. ソースコードの実行場所、各種ファイルの保存場所の確認
- 上記の書誌情報データセット(Excel) とソースコードは同じフォルダにおきます
- Excel ファイル名とソースコード内の Excel ファイル名を同一にします
- Hugging Faceのトークンを設定します
- 各種変数を設定します
  - `working_folder`
  - `hugging_face_repogitory_name`
  - `destination_folder`
  - など

# 4. ポイント
## 4.1. 権利区分 == 保存期間満了 の書誌データのみダウンロードする
著作権が切れている書誌データのみ利用するため、`権利区分 == 保存期間満了` でフィルタリングを行います。

```python
def load_data(self):
    if self.data is None:
        print("Loading Excel file...")
        self.data = pd.read_excel(self.file_path)
        self.data['PID'] = self.data['永続的識別子'].str.extract(r'/(\d+)$')
        self.data = self.data[self.data['権利区分'] == '保護期間満了'].reset_index(drop=True)
        print("Excel file loaded and filtered.")
    return self.data
```

## 4.2. バッチ処理
書誌データが大きいため、メモリオーバーを避けるためにバッチ処理を導入しました。デフォルトのバッチサイズは、以下の通りです。

- tosho_1 = 10000
- tosho_2 = 8000

今回はこれらのサイズでメモリオーバーを回避できました。
また、リスト形式でエクセルファイルを渡せるようにしましたが、処理時間が長くなるため、1ファイルずつ実行する方が良いです。

## 4.3. エラー処理
国会図書館デジタルコレクションからのデータダウンロード時に、ダウンロード失敗、破損した zip ファイルに対応するためのエラー処理を実装しました。

例えば、ダウンロードできない場合は以下のようなエラーメッセージが表示されます。[^5]
```
OS error for PID 785596: 403 Client Error: Forbidden for url: https://lab.ndl.go.jp/dl/api/book/fulltext/785596
```

[^5]: このURL (https://lab.ndl.go.jp/dl/api/book/fulltext/785596) にブラウザでアクセスすると `This PID is not allowed` と表示されます。一方でデジタルコレクション (https://dl.ndl.go.jp/pid/785596) にアクセスすると書誌データは存在します。しかし、画像ファイルのみでテキスト情報が含まれていません。おそらくテキストデータがないため、ダウンロードできないのだと思われます。

## 4.4. 再実行時のスキップ処理
処理が途中で止まり、再度プログラムを実行する際、ダウンロード済みテキストはダウンロードをスキップしてテキスト読み込み処理をします。

```python
# 対象フォルダ内にすでに{pid}.txtがある場合はテキストを読み込む
if os.path.exists(os.path.join(destination_folder, f"{pid}.txt")):
    with open(os.path.join(destination_folder, f"{pid}.txt"), 'r') as file:
        content = file.read()
    return content
```

# 5. まとめ
Google Colab を使って国会図書館のデータを取得し、バッチ処理やメモリ管理の工夫をしながらデータを処理する方法を紹介しました。また、作業には数時間〜十数時間かかることや、Google Driveに約400GBの空き容量が必要な点にも注意が必要です。

:::message
この成果は、NEDO（国立研究開発法人新エネルギー・産業技術総合開発機構）の助成事業「ポスト５Ｇ情報通信システム基盤強化研究開発事業」（JPNP20017）の結果得られたものです。
:::
