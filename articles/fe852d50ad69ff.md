---
title: "国会図書館データをGoogle Colab で取得し、Hugging Face へアップロードする方法"
emoji: "📚"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["LLM", "GENIAC", "松尾研", "huggingface", "国会図書館"]
published: false
# publication_name: matsuolab
---

# 1. はじめに
NEDO[^1] GENIAC 松尾研 LLM開発プロジェクト[^2] の Team Tanuki の事前学習データ収集の一環として、国会図書館デジタルコレクション[^3]のデータをダウンロードし Hugging Face へアップロードしました。本記事では実際に用いたソースコードを元に、データ処理において気を付けるべきポイントについて説明します。

[^1]: NEDO 国立研究開発法人新エネルギー・産業技術総合開発機構
[^2]: GENIAC 松尾研 LLM開発プロジェクト  
    https://weblab.t.u-tokyo.ac.jp/geniac_llm/

[^3]: 国会図書館デジタルコレクション  
    https://dl.ndl.go.jp

# 2. 前処理
## 2.1. 動作環境
- Google Colab Pro
- GPU: 不要
- メモリ: `ハイメモリ`モード (50GB 程度必要)
- ディスク空き容量: 400GB 程度

## 2.2. 書誌情報データセットの入手
国会図書館オープンデータセット[^4] から書誌情報データセットをダウンロードします。  
本記事のソースコードはエクセルファイルをベースに作成しており、今回のプロジェクトでは以下4つのファイルを利用しました。

- 図書 x 2ファイル
- 古典籍
- 雑誌

[^4]: 国会図書館オープンデータセット  
    https://www.ndl.go.jp/jp/dlib/standards/opendataset/index.html

## 2.3. ファイル実行場所
上記のエクセルとソースコードは同じフォルダにおいた状態にしてください。


# 3. ソースコード
ソースコードはこちらからダウンロードできます。  
https://github.com/matsuolab/nedo_project_code/blob/team_hatakeyama_phase2/team_hatakeyama_phase2/pre_training_data/ndlj/text_download_NationalDietLibraryJapan.ipynb

このコードは次のように処理されます。

1. データの読み込み: 書誌情報データセットのエクセルを読み込みます。
2. データのダウンロード: 国会図書館デジタルコレクションから書誌データを一つずつダウンロードします。
3. データの保存: ダウンロードしたデータを "本文" という列で保存します。
4. データのアップロード: 処理したデータを Hugging Face へアップロードします。

# 4. ポイント
## 4.1. 権利区分 == 保存期間満了 の書誌データのみダウンロードする
著作権が切れている書誌データのみ利用するため、`権利区分 == 保存期間満了` でフィルタリングを行います。

```python
def load_data(self):
    if self.data is None:
        print("Loading Excel file...")
        self.data = pd.read_excel(self.file_path)
        self.data['PID'] = self.data['永続的識別子'].str.extract(r'/(\d+)$')
        self.data = self.data[self.data['権利区分'] == '保護期間満了'].reset_index(drop=True)
        print("Excel file loaded and filtered.")
    return self.data
```

## 4.2. バッチ処理
書誌データが大きいため、メモリオーバーを避けるためにバッチ処理を導入しました。適切なバッチサイズは、以下の通りです。

- tosho_1 = 10000
- tosho_2 = 8000

これらのサイズでメモリオーバーを回避できます。
また、リスト形式でエクセルファイルを渡せるようにしましたが、処理時間が長くなるため、1ファイルずつ実行する方が効率的です。

## 4.3. エラー処理
国会図書館デジタルコレクションからのデータダウンロード時に、ダウンロード失敗、破損した zip ファイルに対応するためのエラー処理を実装しました。

例えば、ダウンロードできない場合は以下のようなエラーメッセージが表示されます。[^5]
```
OS error for PID 785596: 403 Client Error: Forbidden for url: https://lab.ndl.go.jp/dl/api/book/fulltext/785596
```

[^5]: このURL (https://lab.ndl.go.jp/dl/api/book/fulltext/785596) にブラウザでアクセスすると `This PID is not allowed` と表示されます。一方でデジタルコレクション (https://dl.ndl.go.jp/pid/785596) にアクセスすると書誌データは存在します。しかし、画像ファイルのみでテキスト情報が含まれていません。おそらくテキストデータがないため、ダウンロードできないのだと思われます。

## 4.4. 再実行時のスキップ処理
処理が途中で止まり、再度プログラムを実行する際、一度ダウンロードしたテキストのダウンロードはスキップしてテキスト読み込み処理をします。

```python
# 対象フォルダ内にすでに{pid}.txtがある場合はテキストを読み込む
if os.path.exists(os.path.join(destination_folder, f"{pid}.txt")):
    with open(os.path.join(destination_folder, f"{pid}.txt"), 'r') as file:
        content = file.read()
    return content
```

# 5. まとめ
Google Colab を使って国会図書館のデータを効率よく取得し、バッチ処理やメモリ管理の工夫をしながらデータを処理する方法を紹介しました。また、作業には数時間がかかることや、Google Driveに約400GBの空き容量が必要な点にも注意が必要です。

:::message
この成果は、NEDO（国立研究開発法人新エネルギー・産業技術総合開発機構）の助成事業「ポスト５Ｇ情報通信システム基盤強化研究開発事業」（JPNP20017）の結果得られたものです。
:::
